{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/90/553120309c53bdfca25c9c50769ae40a538a90c24db8c082468aec898d00/scikit_image-0.14.1-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.3MB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.17.0 (from scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 31.2MB 2.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.3.0 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from scikit-image) (5.2.0)\n",
      "Collecting dask[array]>=0.9.0 (from scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/73/8ffed9140e343455427e92e6a32c354e9acdbef0a23d0e8d6c336d4947e5/dask-0.19.4-py2.py3-none-any.whl (674kB)\n",
      "\u001b[K    100% |████████████████████████████████| 675kB 38.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from scikit-image) (1.11.0)\n",
      "Collecting cloudpickle>=0.2.1 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/87/7b7ef3038b4783911e3fdecb5c566e3a817ce3e890e164fc174c088edb1e/cloudpickle-0.6.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from scikit-image) (2.2.3)\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/68/74a8527b3a727aa69736baaf5a273d83947fa6c91ef4f2e1efddda00d8b6/PyWavelets-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (4.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.4MB 16.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=1.8 (from scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.7MB 28.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from scipy>=0.17.0->scikit-image) (1.14.5)\n",
      "Collecting toolz>=0.7.3; extra == \"array\" (from dask[array]>=0.9.0->scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/d0/a73c15bbeda3d2e7b381a36afb0d9cd770a9f4adc5d1532691013ba881db/toolz-0.9.0.tar.gz (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 31.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image) (2.7.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image) (2.2.0)\n",
      "Requirement already satisfied: pytz in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image) (2018.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/rl/ml-env/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image) (39.1.0)\n",
      "Building wheels for collected packages: networkx, toolz\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
      "  Running setup.py bdist_wheel for toolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/f4/0c/f6/ce6b2d1aa459ee97cc3c0f82236302bd62d89c86c700219463\n",
      "Successfully built networkx toolz\n",
      "Installing collected packages: scipy, toolz, dask, cloudpickle, PyWavelets, networkx, scikit-image\n",
      "Successfully installed PyWavelets-1.0.1 cloudpickle-0.6.1 dask-0.19.4 networkx-2.2 scikit-image-0.14.1 scipy-1.1.0 toolz-0.9.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "get_label = {\n",
    "    'articulated_truck': 0,\n",
    "    'background': 1,\n",
    "    'bicycle': 2,\n",
    "    'bus': 3,\n",
    "    'car': 4,\n",
    "    'motorcycle': 5,\n",
    "    'non-motorized_vehicle': 6,\n",
    "    'pedestrian': 7,\n",
    "    'pickup_truck': 8,\n",
    "    'single_unit_truck': 9,\n",
    "    'work_van': 10\n",
    "}\n",
    "\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "num_epochs = 2\n",
    "num_classes = 11\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        \n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        \n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return {'image': img, 'label': sample['label']}\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image), 'label': sample['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform = None):\n",
    "        self.transform = transform\n",
    "        self.csv_ds = pd.read_csv(csv_file, dtype='str')\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(self.csv_ds.iloc[idx][0], self.csv_ds.iloc[idx][1])\n",
    "        img_name = str(self.csv_ds.iloc[idx][0])\n",
    "        img_path = os.path.join(self.root_dir, 'train',str(self.csv_ds.iloc[idx][1]), str(self.csv_ds.iloc[idx][0]) + '.jpg')\n",
    "        image = io.imread(img_path)\n",
    "        label = get_label[self.csv_ds.iloc[idx][1]]\n",
    "        sample = {'image': image, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "tds = DS(csv_file='./gt_train.csv', root_dir='./',\n",
    "                                    transform = transforms.Compose([Rescale((100, 100)), ToTensor()]) )\n",
    "dataset_size = len(tds)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(tds, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(tds, batch_size=batch_size,\n",
    "\n",
    "                                                \n",
    "                                                sampler=validation_sampler)\n",
    "\n",
    "# dl = torch.utils.data.DataLoader(dataset=tds, batch_size=10, shuffle=True)\n",
    "\n",
    "# model = CNNModel()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(20000, num_classes+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4154\n",
      "Epoch [1/2], Step [100/4154], Loss: 0.7855\n",
      "Epoch [1/2], Step [200/4154], Loss: 0.7625\n",
      "Epoch [1/2], Step [300/4154], Loss: 0.6630\n",
      "Epoch [1/2], Step [400/4154], Loss: 0.4832\n",
      "Epoch [1/2], Step [500/4154], Loss: 0.4994\n",
      "Epoch [1/2], Step [600/4154], Loss: 0.5163\n",
      "Epoch [1/2], Step [700/4154], Loss: 0.4016\n",
      "Epoch [1/2], Step [800/4154], Loss: 0.4643\n",
      "Epoch [1/2], Step [900/4154], Loss: 0.5043\n",
      "Epoch [1/2], Step [1000/4154], Loss: 0.2977\n",
      "Epoch [1/2], Step [1100/4154], Loss: 0.3598\n",
      "Epoch [1/2], Step [1200/4154], Loss: 0.2509\n",
      "Epoch [1/2], Step [1300/4154], Loss: 0.3706\n",
      "Epoch [1/2], Step [1400/4154], Loss: 0.5253\n",
      "Epoch [1/2], Step [1500/4154], Loss: 0.3112\n",
      "Epoch [1/2], Step [1600/4154], Loss: 0.4270\n",
      "Epoch [1/2], Step [1700/4154], Loss: 0.4334\n",
      "Epoch [1/2], Step [1800/4154], Loss: 0.4352\n",
      "Epoch [1/2], Step [1900/4154], Loss: 0.4108\n",
      "Epoch [1/2], Step [2000/4154], Loss: 0.5355\n",
      "Epoch [1/2], Step [2100/4154], Loss: 0.3222\n",
      "Epoch [1/2], Step [2200/4154], Loss: 0.4537\n",
      "Epoch [1/2], Step [2300/4154], Loss: 0.4307\n",
      "Epoch [1/2], Step [2400/4154], Loss: 0.3162\n",
      "Epoch [1/2], Step [2500/4154], Loss: 0.3559\n",
      "Epoch [1/2], Step [2600/4154], Loss: 0.2378\n",
      "Epoch [1/2], Step [2700/4154], Loss: 0.3651\n",
      "Epoch [1/2], Step [2800/4154], Loss: 0.4655\n",
      "Epoch [1/2], Step [2900/4154], Loss: 0.2740\n",
      "Epoch [1/2], Step [3000/4154], Loss: 0.3158\n",
      "Epoch [1/2], Step [3100/4154], Loss: 0.2953\n",
      "Epoch [1/2], Step [3200/4154], Loss: 0.3745\n",
      "Epoch [1/2], Step [3300/4154], Loss: 0.2510\n",
      "Epoch [1/2], Step [3400/4154], Loss: 0.4820\n",
      "Epoch [1/2], Step [3500/4154], Loss: 0.3135\n",
      "Epoch [1/2], Step [3600/4154], Loss: 0.2591\n",
      "Epoch [1/2], Step [3700/4154], Loss: 0.2493\n",
      "Epoch [1/2], Step [3800/4154], Loss: 0.3241\n",
      "Epoch [1/2], Step [3900/4154], Loss: 0.3062\n",
      "Epoch [1/2], Step [4000/4154], Loss: 0.1665\n",
      "Epoch [1/2], Step [4100/4154], Loss: 0.2574\n",
      "4154\n",
      "Epoch [2/2], Step [100/4154], Loss: 0.2857\n",
      "Epoch [2/2], Step [200/4154], Loss: 0.1914\n",
      "Epoch [2/2], Step [300/4154], Loss: 0.3582\n",
      "Epoch [2/2], Step [400/4154], Loss: 0.2115\n",
      "Epoch [2/2], Step [500/4154], Loss: 0.3226\n",
      "Epoch [2/2], Step [600/4154], Loss: 0.2265\n",
      "Epoch [2/2], Step [700/4154], Loss: 0.3513\n",
      "Epoch [2/2], Step [800/4154], Loss: 0.2761\n",
      "Epoch [2/2], Step [900/4154], Loss: 0.1873\n",
      "Epoch [2/2], Step [1000/4154], Loss: 0.2657\n",
      "Epoch [2/2], Step [1100/4154], Loss: 0.2826\n",
      "Epoch [2/2], Step [1200/4154], Loss: 0.3446\n",
      "Epoch [2/2], Step [1300/4154], Loss: 0.3230\n",
      "Epoch [2/2], Step [1400/4154], Loss: 0.3104\n",
      "Epoch [2/2], Step [1500/4154], Loss: 0.1780\n",
      "Epoch [2/2], Step [1600/4154], Loss: 0.3076\n",
      "Epoch [2/2], Step [1700/4154], Loss: 0.1679\n",
      "Epoch [2/2], Step [1800/4154], Loss: 0.2798\n",
      "Epoch [2/2], Step [1900/4154], Loss: 0.1856\n",
      "Epoch [2/2], Step [2000/4154], Loss: 0.4718\n",
      "Epoch [2/2], Step [2100/4154], Loss: 0.2005\n",
      "Epoch [2/2], Step [2200/4154], Loss: 0.2247\n",
      "Epoch [2/2], Step [2300/4154], Loss: 0.3731\n",
      "Epoch [2/2], Step [2400/4154], Loss: 0.2116\n",
      "Epoch [2/2], Step [2500/4154], Loss: 0.1902\n",
      "Epoch [2/2], Step [2600/4154], Loss: 0.2452\n",
      "Epoch [2/2], Step [2700/4154], Loss: 0.2474\n",
      "Epoch [2/2], Step [2800/4154], Loss: 0.4556\n",
      "Epoch [2/2], Step [2900/4154], Loss: 0.3441\n",
      "Epoch [2/2], Step [3000/4154], Loss: 0.2935\n",
      "Epoch [2/2], Step [3100/4154], Loss: 0.3632\n",
      "Epoch [2/2], Step [3200/4154], Loss: 0.3981\n",
      "Epoch [2/2], Step [3300/4154], Loss: 0.2620\n",
      "Epoch [2/2], Step [3400/4154], Loss: 0.3196\n",
      "Epoch [2/2], Step [3500/4154], Loss: 0.2703\n",
      "Epoch [2/2], Step [3600/4154], Loss: 0.1960\n",
      "Epoch [2/2], Step [3700/4154], Loss: 0.2468\n",
      "Epoch [2/2], Step [3800/4154], Loss: 0.2347\n",
      "Epoch [2/2], Step [3900/4154], Loss: 0.3138\n",
      "Epoch [2/2], Step [4000/4154], Loss: 0.2142\n",
      "Epoch [2/2], Step [4100/4154], Loss: 0.1329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ConvNet(num_classes)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    print(len(train_loader))\n",
    "    for i, x in enumerate(train_loader):\n",
    "        images = Variable(x['image'])\n",
    "        labels = Variable(x['label'])\n",
    "#         print(images.shape, labels)\n",
    "        \n",
    "        outputs = model(images.float())\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if i == 10:\n",
    "#             break\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 83\n",
      "100 92\n",
      "100 89\n",
      "100 85\n",
      "100 87\n",
      "100 89\n",
      "100 87\n",
      "100 92\n",
      "100 82\n",
      "100 88\n",
      "100 84\n",
      "100 85\n",
      "100 83\n",
      "100 92\n",
      "100 88\n",
      "100 90\n",
      "100 92\n",
      "100 95\n",
      "100 88\n",
      "100 89\n",
      "100 84\n",
      "100 88\n",
      "100 87\n",
      "100 81\n",
      "100 90\n",
      "100 89\n",
      "100 78\n",
      "100 93\n",
      "100 90\n",
      "100 87\n",
      "100 87\n",
      "100 77\n",
      "100 83\n",
      "100 85\n",
      "100 90\n",
      "100 88\n",
      "100 85\n",
      "100 89\n",
      "100 84\n",
      "100 81\n",
      "100 90\n",
      "100 86\n",
      "100 87\n",
      "100 89\n",
      "100 88\n",
      "100 83\n",
      "100 88\n",
      "100 89\n",
      "100 94\n",
      "100 90\n",
      "100 88\n",
      "100 88\n",
      "100 89\n",
      "100 86\n",
      "100 92\n",
      "100 88\n",
      "100 82\n",
      "100 88\n",
      "100 85\n",
      "100 85\n",
      "100 86\n",
      "100 83\n",
      "100 86\n",
      "100 89\n",
      "100 90\n",
      "100 86\n",
      "100 90\n",
      "100 84\n",
      "100 91\n",
      "100 89\n",
      "100 91\n",
      "100 89\n",
      "100 85\n",
      "100 82\n",
      "100 80\n",
      "100 89\n",
      "100 87\n",
      "100 89\n",
      "100 91\n",
      "100 88\n",
      "100 91\n",
      "100 87\n",
      "100 90\n",
      "100 91\n",
      "100 85\n",
      "100 83\n",
      "100 89\n",
      "100 83\n",
      "100 89\n",
      "100 90\n",
      "100 91\n",
      "100 75\n",
      "100 81\n",
      "100 89\n",
      "100 87\n",
      "100 87\n",
      "100 86\n",
      "100 96\n",
      "100 91\n",
      "100 86\n",
      "100 87\n",
      "100 88\n",
      "100 91\n",
      "100 87\n",
      "100 83\n",
      "100 83\n",
      "100 88\n",
      "100 86\n",
      "100 86\n",
      "100 90\n",
      "100 89\n",
      "100 86\n",
      "100 89\n",
      "100 87\n",
      "100 84\n",
      "100 84\n",
      "100 88\n",
      "100 87\n",
      "100 86\n",
      "100 86\n",
      "100 88\n",
      "100 89\n",
      "100 87\n",
      "100 87\n",
      "100 87\n",
      "100 91\n",
      "100 88\n",
      "100 89\n",
      "100 89\n",
      "100 91\n",
      "100 87\n",
      "100 90\n",
      "100 88\n",
      "100 91\n",
      "100 83\n",
      "100 83\n",
      "100 86\n",
      "100 89\n",
      "100 92\n",
      "100 86\n",
      "100 92\n",
      "100 88\n",
      "100 82\n",
      "100 90\n",
      "100 88\n",
      "100 90\n",
      "100 85\n",
      "100 87\n",
      "100 91\n",
      "100 89\n",
      "100 87\n",
      "100 87\n",
      "100 94\n",
      "100 85\n",
      "100 90\n",
      "100 82\n",
      "100 85\n",
      "100 94\n",
      "100 83\n",
      "100 93\n",
      "100 86\n",
      "100 82\n",
      "100 85\n",
      "100 84\n",
      "100 90\n",
      "100 87\n",
      "100 87\n",
      "100 88\n",
      "100 83\n",
      "100 89\n",
      "100 86\n",
      "100 89\n",
      "100 88\n",
      "100 94\n",
      "100 92\n",
      "100 86\n",
      "100 89\n",
      "100 88\n",
      "100 93\n",
      "100 88\n",
      "100 91\n",
      "100 87\n",
      "100 87\n",
      "100 83\n",
      "100 88\n",
      "100 85\n",
      "100 85\n",
      "100 87\n",
      "100 84\n",
      "100 88\n",
      "100 89\n",
      "100 90\n",
      "100 85\n",
      "100 83\n",
      "100 87\n",
      "100 89\n",
      "100 89\n",
      "100 88\n",
      "100 92\n",
      "100 87\n",
      "100 91\n",
      "100 93\n",
      "100 90\n",
      "100 89\n",
      "100 87\n",
      "100 91\n",
      "100 84\n",
      "100 87\n",
      "100 89\n",
      "100 89\n",
      "100 83\n",
      "100 90\n",
      "100 85\n",
      "100 95\n",
      "100 87\n",
      "100 89\n",
      "100 87\n",
      "100 90\n",
      "100 89\n",
      "100 85\n",
      "100 90\n",
      "100 88\n",
      "100 87\n",
      "100 85\n",
      "100 87\n",
      "100 90\n",
      "100 86\n",
      "100 92\n",
      "100 91\n",
      "100 87\n",
      "100 89\n",
      "100 81\n",
      "100 86\n",
      "100 80\n",
      "100 84\n",
      "100 86\n",
      "100 87\n",
      "100 86\n",
      "100 86\n",
      "100 88\n",
      "100 92\n",
      "100 87\n",
      "100 87\n",
      "100 86\n",
      "100 93\n",
      "100 85\n",
      "100 87\n",
      "100 86\n",
      "100 87\n",
      "100 92\n",
      "100 86\n",
      "100 89\n",
      "100 93\n",
      "100 89\n",
      "100 90\n",
      "100 88\n",
      "100 84\n",
      "100 85\n",
      "100 94\n",
      "100 82\n",
      "100 87\n",
      "100 90\n",
      "100 85\n",
      "100 90\n",
      "100 83\n",
      "100 87\n",
      "100 91\n",
      "100 86\n",
      "100 94\n",
      "100 85\n",
      "100 92\n",
      "100 84\n",
      "100 89\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-ddb765e83100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#         print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/ml-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/ml-env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-e141b90d58a4>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl/ml-env/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-f5fb405d89b6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# numpy image: H x W x C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# torch image: C X H X W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, x in enumerate(validation_loader):\n",
    "#         print(x)\n",
    "        images = x['image']\n",
    "#         labels_ = []\n",
    "#         for label in labels:\n",
    "#             labels_.append(int(label))\n",
    "#         labels = torch.LongTensor(labels)\n",
    "        labels = x['label']\n",
    "#         print(images, labels)\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(labels.size(0), (predicted == labels).sum().item())\n",
    "\n",
    "\n",
    "    print('Test Accuracy of the model on the ~51057 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
